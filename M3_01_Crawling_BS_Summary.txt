#####################################################################################################
############################################## BeautifullSoup #######################################
#####################################################################################################
######################################################### URL로부터 TAG 정보 가져오기
from bs4 import BeautifulSoup
import requests

url='http://example.com'
response=requests.get(url)                                  ### 아무런 문제 없이 URL 가져오면 200을 출력
#print(response.text)                                       ==> html의 source code 정보
#print(response.content)                                    # 얘는 binary로 가져오니까
#print(response.text)                                       # TEXT로
soup = BeautifulSoup(response.text,'html.parser')           # html parser를 커쳐서 정보를 정제
soup.find_all('html')

##############################################    HEADERS    #######################################
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Accept-Language': 'en-US,en;q=0.9',
    'Referer': 'http://example.com'
}

###########################################    CSS Selector    ####################################
CSS 셀렉터: 크롤링 시 특정 요소를 선택할 때 유용
- #id: 특정 id를 가진 요소를 선택합니다.
- .class: 특정 클래스를 가진 요소를 선택합니다.
- tagname: 특정 태그명을 가진 요소를 선택합니다.


##############################################    XPATH    #######################################
XPath는 XML 문서의 경로를 지정하기 위한 언어로, HTML 문서에서도 사용할 수 있습니다. 예:

```
//div[@id='main']: id가 'main'인 모든 <div> 요소를 선택합니다.
//a/text():
- //: 문서의 어디에서든 지정한 요소를 찾습니다. 즉, 루트 요소부터 시작해서 모든 자식 요소를 포함하여 탐색합니다.
- a: 찾고자 하는 요소의 이름입니다. 여기서는 <a> 태그를 의미합니다.
- /text(): 지정한 요소의 텍스트 노드를 선택합니다. <a> 태그 내부의 텍스트를 의미합니다.
```


######################################### string, get_text, text 비교 #############################
import requests
from bs4 import BeautifulSoup 

html = """
<div class="example">
    Single text node
</div>
<div class="example">
    <p>Multiple</p> text <span>nodes</span>
</div>
"""

soup=BeautifulSoup(html, 'html.parser')
single_text_node=soup.find('div', class_='example').string
multiple=soup.find_all('div', class_='example')

multiple_text_nodes=soup.find_all('div', class_='example')[1].string

print(single_text_node)
print(multiple,'\n\n\n')
print(multiple_text_nodes)          ## None 값 반환

## get_text()로
single_text_node=soup.find('div', class_='example').get_text(strip=True)
multiple=soup.find_all('div', class_='example')
multiple_text_nodes=soup.find_all('div', class_='example')[1].get_text(strip=True)
print(single_text_node)
print(multiple_text_nodes)          ## ==> Multipletextnodes

# get_text() w/ separator
multiple_text_nodes=soup.find_all('div', class_='example')[1].get_text(separator=' | ', strip=True)
# ==> Multiple | text | nodes

### ==> text: multiple node는 사용가능하지만 option은 사용 불가
multiple_text_nodes=soup.find_all('div', class_='example')[1].text


######################################### 정규 표현식 이용 이메일 추출 ##################################
import requests
from bs4 import BeautifulSoup 
import re

html = """
<ul>
<li>Email:example@example.com</li>
<li>Contact:contact@sample.org</li>
</ul>
"""
## 이메일 주소 추출
email_pattern = r'[a-zA-z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
emails=re.findall(email_pattern,html)
print(emails)
######################################### 정규 표현식 이용 text 추출 ##################################
html = '''
<html>
<body>
<div>
Hello,world!
</div>
<div>
<p>
Hello,<b>world!</b>
</p>
</div>
</body>
</html>
'''

soup=BeautifulSoup(html,'html.parser')
bs=soup.body.text
###### for Hello,world!Hello,world!
new_bs=re.sub("\s+", "", bs)

###### for 
###### Hello,world!
###### Hello,world!
texts=re.findall('[^\s]+', bs)
for t in texts:
    print(t)

################################################# urllib.request ##################################
from bs4 import BeautifulSoup
import requests
url='https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100'
response=requests.get(url)
html=response.text
bs=BeautifulSoup(html,'html.parser')
print(bs)

==> 동일하게 urllib.request        ### text를 뽑아낼 필요 없이 바로 parsing 가능
import urllib.request as rq
url='https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100'
html=rq.urlopen(url)
bs=BeautifulSoup(html,'html.parser')
print(bs)

#####################################################################################################
################################################ EXAMPLES ###########################################
#####################################################################################################
html_doc = """
    <html><head><title>The Dormouse's story</title></head>
    <body>
        <p class="title"><b>The Dormouse's story</b></p>
        <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>
        <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a>
        <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>
    </body></html>
"""
################################################################# Q. 모든 <a> 태그 추출 anchor
links=soup.find_all('a')
for link in links:
    print(link.get('href'))
==>
<a href="https://www.iana.org/domains/example">More information...</a>
href ==>
https://www.iana.org/domains/example

################################################################# Q. 모든 <div> 태그 추출
divs=soup.find_all('div')
for div in divs:
    print(div.text)    
==>
[<div>
 <h1>Example Domain</h1>
 <p>This domain is for use in illustrative examples in documents. You may use this
     domain in literature without prior coordination or asking for permission.</p>
 <p><a href="https://www.iana.org/domains/example">More information...</a></p>
 </div>]
div.text ==>
Example Domain
This domain is for use in illustrative examples in documents. You may use this
    domain in literature without prior coordination or asking for permission.
More information...

################################################################# Q. 모든 <h1> 태그 추출
h1s=soup.find_all('h1')
for h1 in h1s:
    print(h1.text)
h1==>
[<h1>Example Domain</h1>]
h1.txt ==>
Example Domain

################################################################# Q. 모든 <meta> 태그 추출
metas=soup.find_all('meta')
for meta in metas:
    print(meta)
    print(meta.attrs)
meta ==> 
[<meta charset="utf-8"/>,
 <meta content="text/html; charset=utf-8" http-equiv="Content-type"/>,
 <meta content="width=device-width, initial-scale=1" name="viewport"/>]
meta.attrs ==> 
{'charset': 'utf-8'}
{'http-equiv': 'Content-type', 'content': 'text/html; charset=utf-8'}
{'name': 'viewport', 'content': 'width=device-width, initial-scale=1'}

################################################################# Q. 클래스가 sister인 모든 태그를 추출
sister_tags = soup.find_all(class_='sister')
# ID가 link1인 태그를 추출
ID_tags = soup.find_all(id='link1')

################################################################# Q. 모든 텍스트를 추출
soup = BeautifulSoup(html_doc,'html.parser')
all_text = soup.get_text()

################################################################# Q. 부모 태그 추출
soup = BeautifulSoup(html_doc, 'html.parser')
tag_link1 = soup.find(id='link1')
print("ID가 link1인 태그: \n")
print(tag_link1)
parent_tag = tag_link1.parent
print("\n\n부모 태그: \n")
print(parent_tag,'\n')
print("부모 태그의 name: \n")
print(parent_tag.name)
print("부모 태그의 class: \n")
print(parent_tag.get('class'))
print("부모 태그의 ID: \n")
print(parent_tag.get('id'))
print("부모 태그의 text: \n")
print(parent_tag.text)

################################################################# Q. 다음 형제 태그 추출
soup = BeautifulSoup(html_doc, 'html.parser')
tag_link1 = soup.find(id='link1')
# 다음 형제 태그 추출
next_sibling_tag = tag_link1.find_next_sibling()
print("ID가 link1인 태그의 다음 형제 태그: \n")
print(next_sibling_tag)
print("다음 형제 태그의 name: \n")
print(next_sibling_tag.name)
print("다음 형제 태그의 class: \n")
print(next_sibling_tag.get('class'))
print("다음 형제 태그의 ID: \n")
print(next_sibling_tag.get('id'))
print("다음 형제 태그의 text: \n")
print(next_sibling_tag.text)

################################################################# Q. 이전 형제 태그 추출
soup = BeautifulSoup(html_doc, 'html.parser')
tag_link2 = soup.find(id='link2')
prev_sibling_tag = tag_link2.find_previous_sibling()
print("ID가 link2인 태그의 이전 형제 태그: \n")
print(prev_sibling_tag)
print("이전 형제 태그의 name: \n")
print(prev_sibling_tag.name)
print("이전 형제 태그의 class: \n")
print(prev_sibling_tag.get('class'))
print("이전 형제 태그의 ID: \n")
print(prev_sibling_tag.get('id'))
print("이전 형제 태그의 text: \n")
print(prev_sibling_tag.text)

################################################################# Q. 부모 태그가 같은지 확인
soup = BeautifulSoup(html_doc, 'html.parser')
## <p>, <a> 태그의 부모 태그 확인
p_tag=soup.find('p')
a_tag=soup.find('a')
parent_p=p_tag.parent.name
parent_a=a_tag.parent.name
are_siblings=p_tag.find_next_sibling() == a_tag
print(f"<p> 태그의 부모 태그: {parent_p}")
print(f"<a> 태그의 부모 태그: {parent_a}")
print(f"<p> 태그와 <a> 태그는 형제 태그인가요? {'예' if are_siblings else '아니요'}")

################################################################# Q. 자식 태그 확인
p_tag=soup.find('p', class_='title')
print(p_tag)
children_p=p_tag.children
for child in children_p:
    print(child.name)

################################################################# Q. 주어진 HTML 문서에서 ID가 link1인 태그의 href 속성 값을 추출
soup = BeautifulSoup(html_doc, 'html.parser')
tag_link1 = soup.find(id='link1')
#href_value = tag_link1.get('href')
href_value = tag_link1['href']
print("ID가 link1인 태그의 href 속성 값: \n")
print(href_value)
################################################################# Q.주어진 html 문서에서 모든 링크의  url을 추출
soup=BeautifulSoup(html_doc, 'html.parser')
a_tags=soup.select('.sister')
for tag in a_tags:
    print(tag['href'])
==>
http://example.com/elsie
http://example.com/lacie
http://example.com/tillie
################################################################# Q.주어진 html 문서에서 ID가 link1인 태그를 추출
link1_tag=soup.select_one('#link1')
print(link1_tag)

################################################################# Q.주어진 html 문서에서 <p> 태그의 모든 자식 태그를 추출
p_tag=soup.select_one('p')
children=p_tag.findChildren()
for child in children:
    print(child)

################################################################# Q.주어진 html 문서에서 ID가 link1인 태그의 부모 태그 추출
link1_tag=soup.select_one('#link1')
print(link1_tag.find_parent())

################################################################# Q.주어진 html 문서에서 ID가 link1인 태그의 다음 형제 태그 추출
link1_tag=soup.select_one('#link1')
print(link1_tag.find_next_sibling())

print(link1_tag.find_previous_sibling())

######################################################## XPATH #############################################################
from lxml import html

html_doc = """
<html>
  <head><title>The Dormouse's story</title></head>
  <body>
    <p class="title"><b>The Dormouse's story</b></p>
    <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>
    <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a>
    <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>
  </body>
</html>
"""
################################################################# Q.<a> tag의 모든 text 추출
tree=html.fromstring(html_doc)      # html 문서를 xml 요소로 변환
a_texts=tree.xpath('//a/text()')
for text in a_texts:
    print(text)

################################################################# Q.주어진 html 문서에서 ID가 link1인 태그를 추출
link1_tag=tree.xpath("//*[@id='link1']")[0]
print(html.tostring(link1_tag).decode('utf-8'))
==>
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>
################################################################# Q.네이버 사이트에서 ID가 'wrap'인 영역 크롤링
import requests
from bs4 import BeautifulSoup
url='https://www.naver.com/'
response=requests.get(url)
print(response.status_code)
soup = BeautifulSoup(response.text, 'html.parser')
wrap=soup.find(id='wrap')                               # ID를 사용하여 'wrap' 영역 크롤링
#print(wrap)
print(wrap.prettify())                                  # 좀 더 정렬해서 보기 좋게


################################################################# Q.네이버 사이트에서 <a> 태그의 텍스트와 href 속성 추출
url='https://www.naver.com/'
response=requests.get(url)
print(response.status_code)
soup = BeautifulSoup(response.text, 'html.parser')
links_a=soup.find_all('a')
for link in links_a:
    print("#######################")
    print(link.get_text())          # link.text로 해도 된다
    print(link.get('href'))

################################################################# Q.네이버 사이트에서 search_area class와 ID로 찾기
url='https://www.naver.com/'
response=requests.get(url)
print(response.status_code)
soup = BeautifulSoup(response.text, 'html.parser')

class_tag=soup.select_one('.search_area')
print(class_tag)

ID_tag=soup.select_one('#search_area')
print(ID_tag)

################################################################# Q. 네이버 사이트에서 search_area class와 ID로 찾기
search_area=soup.find(class_='search_area')
print(search_area)
print(search_area.prettify())

################################################################# Q.# 네이버 사이트에서 data-gfp-banner-size 속성을 가진 추출
# data-gfp-banner-size 자체가지고는 TAG가 아니라 속성이라 아래 처럼 Dict로 crawling

#banners=soup.find_all(attrs={'data-gfp-banner-size': '830x130'})
banners=soup.find_all(attrs={'id': 'ad-timeboard-response'})
for banner in banners:
    print(banner.prettify())

################################################################# Q. 네이버 사이트에서 ID가 'u_skip'인 div 내부의 모든 a 태그 선택
u_skip_links=soup.select('#u_skip a')             ## find_all, find는 TAG의 속성가지고 찾을때, select/select_on은 CSS select 으로 찾을 때 사용
for link in u_skip_links:
    print(link.text)
################################################################# Q.## 네이버 사이트에서 ID가 'wrap'인 div 내부의 ID가 'header'인 div 선택
header_div=soup.select_one('#wrap #header')          
print(header_div.text if header_div else "no header div found")

################################################################# Q.# 네이버 사이트에서 ID가 'ad-timeboard-response'인 script 태그의 data-gfp-banner-size 속성값 선택
ad_timeboard_response=soup.select_one('script#ad-timeboard-response')
banner_size=ad_timeboard_response['data-gfp-banner-size'] if ad_timeboard_response else 'No data-gfp-banner-size found'
print(banner_size)


################################################################# Q. 네이버 뉴스 추출

import requests
from bs4 import BeautifulSoup 

## 예시 네이버 뉴스 페이지 URL
url='https://news.naver.com'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Accept-Language': 'en-US,en;q=0.9',
    'Referer': 'http://example.com'
}
response=requests.get(url, headers=headers)

soup=BeautifulSoup(response.text, 'html.parser')
print(soup.prettify())
### 네이버 뉴스 찾기
## title 태그만으로 찾기
news_titles=soup.find('title')
news_titles.text
news_titles.get_text()
news_titles.string
#tag1=soup.select('#browserTitleArea')
#print(tag1[0].text.strip())   

################################################################# Q.### 네이버 뉴스에서 언론사별 ... 카테고리 출력
cats=soup.find_all(class_='Nitem_link_menu')
for idx,cat in enumerate(cats):
    print(f"{idx+1}: {cat.get_text().strip()}")

################################################################# Q.다음 사항을 수행
- 첫 번째로 매칭되는 'p' 태그 찾기
- 모든 'p' 태그 찾기
- 클래스가 'content'인 첫 번째 'p' 태그 찾기
- 클래스가 'content'인 모든 'p' 태그 찾기
- 특정 'p' 태그의 모든 부모 태그 찾기
- 특정 'p' 태그의 첫 번째 부모 태그 찾기
- 특정 'p' 태그의 다음 형제 태그 찾기
- 특정 'p' 태그의 이전 형제 태그 찾기
- 특정 'p' 태그 다음에 위치한 모든 태그나 문자열 찾기
- 특정 'p' 태그 이전에 위치한 모든 태그나 문자열 찾기
import requests
from bs4 import BeautifulSoup 

html_contents='<html><body><h1>Title</h1><p class="content">First paragraph.</p><p class="content">Second paragraph.</p></body></html>'
soup=BeautifulSoup(html_contents,'html.parser')
print(soup.prettify())
## 1
print("\n#1 첫 번째 'p' 태그")
tag=soup.find('p')
print(tag)
print(tag.get_text())

## 2
print("\n#2 모든 'p' 태그")
tags=soup.find_all('p')
for tag in tags:
    print(tag)

## 3
print("\n#3 클래스가 'content'인 첫 번째 'p' 태그")
content_p=soup.find('p', class_='content')      
print(content_p)
# 강사님 답안
first_content_p=soup.select_one('p.content')
print(first_content_p.get_text())

## 4
print("\n#4 클래스가 'content'인 모든 'p' 태그")
content_ps=soup.find_all('p', class_='content')      
for content_p in content_ps:
    print(content_p)
# 강사님 답안
all_content_p=soup.select('p.content')
for content_p in all_content_p:
    print(content_p.get_text())
    
## 5 [doc] ==> <html> ==> <body> ==> <p> : 최상위 요소에서 p태그까지 모든 부모 요소를 출력하게 됨
print("\n#5 'p' 태그의 모든 부모 태그")
content_ps=soup.find_all('p')      
for content_p in content_ps:
    parents=content_p.find_parents()                    # find_parents() vs. find_parent()
    print(f"{content_p}의 부모 태그: {parents}")

## 6
print("\n#6 'p' 태그의 첫번째 부모 태그")
content_ps=soup.find_all('p')      
for content_p in content_ps:
    parent=content_p.find_parent()
    print(f"{content_p}의 부모 태그: {parent}")

## 7
print("\n#7 'p' 태그의 다음 형제 태그")
content_ps=soup.find_all('p')      
for content_p in content_ps:
    next_sib=content_p.find_next_sibling()
    print(f"{content_p}의 다음 형제 태그: {next_sib}")

## 8
print("\n#8 'p' 태그의 이전 형제 태그")
content_ps=soup.find_all('p')      
for content_p in content_ps:
    prev_sib=content_p.find_previous_sibling()
    print(f"{content_p}의 이전 형제 태그: {prev_sib}")

## 9
print("\n#9 'p' 태그의 다음에 위치한 모든 태그나 문자열 찾기")
content_ps=soup.find_all('p')      
for content_p in content_ps:
    next_sibs=content_p.find_next_siblings()
    print(f"{content_p}의 다음 형제 태그: {next_sibs}")

## 10
print("\n#10 'p' 태그의 이전에 위치한 모든 태그나 문자열 찾기")
content_ps=soup.find_all('p')      
for content_p in content_ps:
    prev_sibs=content_p.find_previous_siblings()
    print(f"{content_p}의 이전 형제 태그: {prev_sibs}")

###################### 강사님 답안
## 9
print("\n#9 'p' 태그의 다음에 위치한 모든 태그나 문자열 찾기")
specific_p=soup.find('p', class_='content')      
nexts=specific_p.find_next()
print(f"{specific_p}의 다음 모든 태그, 문자열: {nexts}")

## 10
print("\n#10 'p' 태그의 이전에 위치한 모든 태그나 문자열 찾기")
specific_p=soup.find('p', class_='content')      
prev=specific_p.find_previous()
print(f"{specific_p}의 이전 모든 태그, 문자열: {prev}")

################################################################# Q. ID를 이용해서 '네이버 뉴스' 추출
url='https://news.naver.com'
response=requests.get(url, headers=headers)
soup=BeautifulSoup(response.text,'html.parser')
NaverNews=soup.select_one('#browserTitleArea')
## 강사님
NaverNews=soup.find('title', id='browserTitleArea')
NaverNews=soup.select_one('title#browserTitleArea')
print(NaverNews.get_text())
################################################################# Q. 네이버 뉴스에서 언론사별 ... 카테고리 출력 select 이용
cats=soup.select('.Nitem_link_menu')
for idx,cat in enumerate(cats):
    print(f"{idx+1}: {cat.get_text().strip()}")
    
################################################################# Q.select_one을 이용해서 'https://news.naver.com'에서 "뉴스"를 출력
url = 'https://news.naver.com'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

#element = soup.select_one('a[href*="news.naver.com"]')
# 강사님
element=soup.select_one('span.Nicon_service')              # span 태그에서 Nicon_service 클래스

if element:
    print(element.text.strip())
else:
    print("해당 요소를 찾을 수 없습니다.")
################################################################# Q.'https://news.naver.com'에서 아래 예시와 같이 뉴스 기사 제목을 모두 출력 (예시: 1: [속보] '훈련병 사망' 얼차려 지시~)
headlines = soup.find_all(class_='cjs_t')
i=1
for headline in headlines:
    print(f"{i}: {headline.text.strip()}")
    i+=1
## 강사님
headlines = soup.select('.cjs_t')
for idx,headline in enumerate(headlines):
    print(f"{idx+1}: {headline.get_text().strip()}")
    
################################################################# Q.주어진 html에서 아래 사항을 수행
# 모든 단어 추출
# 이메일 주소 추출
# URL 주소
# 숫자 추출
# HTML 태크 내 텍스트 추출

html = '''
<html>
<head>
<title>Sample Page</title>
</head>
<body>
<div>
Hello, world! 123
</div>
<div>
<p>
456 Hello, <b>789 world!</b> Visit us at <a href="http://example.com">example.com</a>
</p>
</div>
<footer>
Contact us at <a href="mailto:info@example.com">info@example.com</a>
</footer>
</body>
</html>
'''

# 1. 모든 단어 추출
print("1. 모든 단어 추출")
soup=BeautifulSoup(html,'html.parser')
words=re.findall(r'\b\w+\b',soup.text)
words_list=''
for i in words:
    words_list += i
    words_list += ' '
print(words_list)

# 2. 이메일 주소 추출
email_pattern = r'[a-zA-z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
emails=re.findall(email_pattern,html)
print("\n2. 이메일 주소 추출")
for email in emails:
    print(email)

# 3. URL 주소 http://example.com
print("\n3. URL 주소 추출")
url_pattern = r'https?://[^\s<>"]+|www\.[^\s<>"]+'
#url_pattern = r'https?:\/\/[^\s/$.?#].[^\s]*'
urls = re.findall(url_pattern, html)
print(urls)

# 4. 숫자 추출
print("\n4. 숫자 추출")
num_pattern = r'\b\d+\b'
nums=re.findall(num_pattern,html)
num_list=''
for num in nums:
    num_list += num
    num_list += ' '
print(num_list)

# 5. HTML 태크 내 텍스트 추출
print("\n5. 텍스트 추출")
soup=BeautifulSoup(html,'html.parser')
bs=soup.body.text

new_bs=re.sub("\s+", " ", bs)
print(new_bs.strip())

## 강사님
print("\n강사님 5. 텍스트 추출")
tag_texts=re.findall(r'>([^<]+)<',html)
text_lists=' '.join(text.strip() for text in tag_texts)
text_lists=re.sub(r'\s+', ' ', text_lists)
print(text_lists.strip())

################################################################# Q. soup.find_all: href의 속성에 정규 표현식 사용
import requests
from bs4 import BeautifulSoup 
import re

html = """
<ul>
  <li><a href="hoge.html">hoge</li>
  <li><a href="https://example.com/fuga">fuga*</li>
  <li><a href="https://example.com/foo">foo*</li>
  <li><a href="http://example.com/aaa">aaa</li>
</ul>
"""
soup=BeautifulSoup(html,'html.parser')
li=soup.find_all(href=re.compile(r"^https://"))
for e in li:print(e.attrs['href'])

################################################################# Q. # 한글만 출력
texts=response.text
result=re.findall('[가-힣]+', texts)
print(' '.join(result))

################################################################# Q.inside out 제목 가져오기
import requests
import chardet

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Accept-Language': 'en-US,en;q=0.9',
    'Referer': 'http://example.com'
}

url = "https://serieson.naver.com/v3/movie/ranking/realtime"

response=requests.get(url, headers=headers)
if response.status_code==200:
    encoding=chardet.detect(response.content)['encoding']
    soup = BeautifulSoup(response.text,'html.parser')  
else:
    print("HTTP request failed")
#print(soup.prettify())
#<span class="Title_title__s9o0D">인사이드 아웃(패키지상품 : 더빙판 + 부가영상 추가증정)</span>
#insideout=soup.select_one('img.Thumbnail_image__TxHd0')
#print(insideout['alt'])
insideout=soup.select_one('span.Title_title__s9o0D')
print(insideout.text)
print(re.split('\(',insideout.text)[0])

################################################################# Q. (위의 질문과 연장) 제목 여러개 추출 인덱싱까지..
#1
titles=soup.select('span.Title_title__s9o0D')
for idx, title in enumerate(titles):
    print(f"{idx+1}: {title.text}")
    if idx==9: break
#2
print('\n')
title_list=[]
for idx, title in enumerate(titles):
    title_list.append(title.text)

for idx, title in enumerate(set(title_list)): 
    print(f"{idx+1}: {title}")
    if idx == 9: break
#3
print('\n')
titles=soup.select('span.Title_title__s9o0D')
for idx, title in enumerate(titles[:10]):
    print(f"{idx+1}: {title.text}")

################################################################# Q. 정치면에서 헤드라인 기사 제목 crawling해와서 DF로 출력
url = "https://news.daum.net/politics#1"

response=requests.get(url, headers=headers)
if response.status_code==200:
    encoding=chardet.detect(response.content)['encoding']
    soup = BeautifulSoup(response.text,'html.parser')  
else:
    print("HTTP request failed")

links=soup.select('a.link_txt')
print(links)
for idx, link in enumerate(links): pass
    #print(f"{idx+1}: {link.get_text()}")
articles=[line.text.strip() for line in links]
df=pd.DataFrame({"Articles":articles})
#df=pd.DataFrame({"Articles":links})
df.head(len(df))

################################################################# Q. copy/copy.selector 이용해서 아래와 같이 crawling 가능
## 해당 title에서 F12/선택활성화/copy/copy.selector 로 가져오면 아래와 같음
# body > div.container-doc.cont-category > main > section > div.main-sub > div.box_g.box_news_major > ul > li:nth-child(1) > strong > a
titles=soup.select('body > div > main > section > div > div > ul > li > strong > a')
for i, title in enumerate(titles, start=1):
    print(f"{i}. {title.get_text().strip()}")
################################################################# Q.위 예제에서 한글만 가져오기
titles=soup.select('body > div > main > section > div > div > ul > li > strong > a')
for i, tag in enumerate(titles, start=1):
    text=tag.get_text().strip().replace(',', '')
    matches = re.findall('[가-힣0-9]+',text)
    print(f"{i}, {' '.join(matches)}")

################################################################# Q. Query + Crawling 함께 진행하기
# news_url.format(query): URL 문자열 내의 특정 부분을 query 변수의 값으로 대체
# - news_url 변수에는 포맷 문자열 'https://news.example.com/search?query={}'가 저장
# - news_url.format(query)는 포맷 문자열의 {} 부분을 query 변수의 값으로 대체
# - 결과적으로, 포맷팅된 URL은 'https://news.example.com/search?query=politics'가 된다.

import requests
import chardet
import pandas as pd
import re
from datetime import datetime
import os

query=input('검색 키워드를 입력하세요: ')
query=query.replace(' ', '+')
print(query)

news_url='https://search.naver.com/search.naver?where=nexearch&sm=tab_jum&query={}'
req=requests.get(news_url.format(query))

html=req.text
soup=BeautifulSoup(html, 'html.parser')
links=soup.select('.news_tit')

for link in links:
    title=link.text
    url=link.attrs['href']
    print(title, '\n', url)


################################################################# Q. Query + Crawling 함께 진행하기 + urllib 사용
# 네이버 뉴스 제목과 링크 추출
# urllib  사용
import urllib.request
import urllib.parse
from bs4 import BeautifulSoup

baseUrl = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query='
plusUrl = input('검색어를 입력하세요: ')

url = baseUrl + urllib.parse.quote_plus(plusUrl)

html = urllib.request.urlopen(url).read()
soup = BeautifulSoup(html, 'html.parser')

title = soup.find_all(class_ = 'news_tit')

for i in title :
    print(i.attrs['title'])
    print(i.attrs['href'])
    print()
    

################################################################# Q.네이버 영화 순위 사이트에서 영화제목, 가격, 타입 DF로 저장
####### option 1
import requests
import pandas as pd
from bs4 import BeautifulSoup 
import chardet

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Accept-Language': 'en-US,en;q=0.9',
    'Referer': 'http://example.com'
}

url = "https://serieson.naver.com/v3/movie/ranking/realtime"

response=requests.get(url, headers=headers)
if response.status_code==200:
    encoding=chardet.detect(response.content)['encoding']
    soup = BeautifulSoup(response.text,'html.parser')  
else:
    print("HTTP request failed")

titles=soup.select('span.Title_title__s9o0D')
types=soup.select('span.Price_text__pRk_f')
prices=soup.select('span.Price_price__GqXqo')
title_li=[]; type_li=[]; price_li=[];
for i in range(len(titles)):
    title_li.append(titles[i].text)
    type_li.append(types[i].text)
    price_li.append(prices[i].text.replace('캐시',''))
df=pd.DataFrame({'Titles':title_li, 'Types':type_li, 'Prices':price_li})
df.head(100)
####### Other Option using Copy/copy.selector

titles = []
types = []
prices = []
movie_tags = soup.select('div > ol > li')
for tag in movie_tags:
    title_tag = tag.select_one('a > div > span')
    if title_tag:
        titles.append(title_tag.text.strip())
    else:
        titles.append(None)
    type_tag = tag.select_one('a > div > div > div > div > div > span.Price_text__pRk_f')
    if type_tag:
        types.append(type_tag.text.strip())
    else:
        types.append(None)

    price_tag = tag.select_one('a > div > div > div > div > div > span.Price_price__GqXqo')
    if price_tag:
        prices.append(price_tag.text.strip().replace('캐시',''))
    else:
        prices.append(None)

df = pd.DataFrame({
    'Title': titles,
    'Type': types,
    'Price': prices
})
df
################################################################# Q. url = 'https://news.daum.net/politics#1'은 정치기사 1페이지인데 10페이지에 있는 기사를 모두 출력
#"https://news.daum.net/politics#1"
#(request랑 BS만 selenium 쓰지말고)
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Accept-Language': 'en-US,en;q=0.9',
    'Referer': 'http://example.com'
}

articles_li=[]

for i in range(1,11, 1):
    url = f"https://news.daum.net/politics#{i}"
    response=requests.get(url, headers=headers)
    if response.status_code==200:
        encoding=chardet.detect(response.content)['encoding']
        soup = BeautifulSoup(response.text,'html.parser')  
    else:
        print("HTTP request failed")

    links=soup.select('a.link_txt')
    articles=[line.text.strip() for line in links]
    articles_li.extend(articles)


df=pd.DataFrame({"Articles":articles_li})
#df=pd.DataFrame({"Articles":links})
df.tail()
df.head()
################################################################# Q. 앞에서 출력한 기사 리스트를 pandas 데이터프레임으로 변환 후 csv 파일로 저장 후 다시 불러오세요.
# (컬럼은 인덱스 번호화 텍스트 2개 컬럼으로 구성된 DF)
import requests
import chardet
import pandas as pd

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Accept-Language': 'en-US,en;q=0.9',
    'Referer': 'http://example.com'
}

url = "https://news.daum.net/politics#1"

response=requests.get(url, headers=headers)
if response.status_code==200:
    encoding=chardet.detect(response.content)['encoding']
    soup = BeautifulSoup(response.text,'html.parser')  
else:
    print("HTTP request failed")

titles=soup.select('body > div > main > section > div > div > ul > li > strong > a')
indexes_li=[]; titles_li=[];
for i, tag in enumerate(titles, start=1):
    text=tag.get_text().strip().replace(',', '')
    matches = re.findall('[가-힣0-9]+',text)
    #print(f"{i}, {' '.join(matches)}")
    indexes_li.append(i); titles_li.append(' '.join(matches))

df_titles=pd.DataFrame({'indexes':indexes_li, 'titles':titles_li})
#df_titles.head()  
df_titles.to_csv('articles.csv', index=False)
df_reloaded=pd.read_csv('articles.csv',index_col=0)
df_reloaded.head()

################################################################# Q. Electro 사이트를 크롤링해서 category, name,   link, price 4개 컬럼으로 구성되는 데이터프레임을 출력하세요.

####### Option-1
import requests
from bs4 import BeautifulSoup 
import pandas as pd
import chardet
import re


headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

categories=[]; names = []; links = []; prices = []
for i in range(1,5):
    url = f'https://startcoding.pythonanywhere.com/basic?page={i}&keyword='
    response=requests.get(url, headers=headers)
    if response.status_code==200:
        encoding=chardet.detect(response.content)['encoding']
        soup = BeautifulSoup(response.text,'html.parser')  
    else:
        print("HTTP request failed")

    category=soup.select('p.product-category')
    name=soup.select('h3.product-name')
    price=soup.select('h4.product-price')
    link=soup.select('h3.product-name a')
    for i in range(len(name)):
        names.append(name[i].get_text().strip())
        categories.append(category[i].text)
        links.append(link[i].attrs['href'])
        #price1=re.split('\s+',price[i].text)[0]
        #prices.append(re.sub('[원\s,\t\n]+','',price1.strip()))
        price1=re.split('\s+',price[i].text)[0]
        prices.append(price1.strip().replace('원','').replace(',',''))
df=pd.DataFrame({'ItemNames':names, 'Categories':categories, 'links':links, 'Price':prices})
df.head()

####### Option-2
import requests
from bs4 import BeautifulSoup 
import pandas as pd
import chardet
import re


headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

categories=[]; names = []; links = []; prices = []
for i in range(1,5):
    url = f'https://startcoding.pythonanywhere.com/basic?page={i}'
    response=requests.get(url, headers=headers)
    if response.status_code==200:
        encoding=chardet.detect(response.content)['encoding']
        soup = BeautifulSoup(response.text,'html.parser')  
    else:
        print("HTTP request failed")
    products=soup.select('div.product')
    for product in products:
        category=product.select_one('p.product-category').text
        name=product.select_one('a').text
        link=product.select_one('a').attrs['href']
        price1=soup.select_one('h4.product-price').text.strip()
        price=re.split('\s+',price1)[0].replace('원','').replace(',','')
        categories.append(category); names.append(name); links.append(link); prices.append(price)
        
df=pd.DataFrame({'ItemNames':names, 'Categories':categories, 'links':links, 'Price':prices})
df.head()
df.to_csv('result.csv', encoding='utf-8-sig',index=False)
df_reloaded=pd.read_csv('result.csv',index_col=0)
df_reloaded.head()

################################################################# Q. 검색 query로 뉴스 타이틀, 링크 저장
#query=input('검색 키워드를 입력하세요: ')
query='인공지능'
query=query.replace(' ', '+')
print(query)
news_url='https://search.naver.com/search.naver?where=nexearch&sm=tab_jum&query={}'

for i in range(1,6):
    url=news_url.format(query)+'&start=' + str(i)
    print(url)
    req=requests.get(news_url.format(query))

    html=req.text
    soup=BeautifulSoup(html, 'html.parser')
    links=soup.select('.news_tit')

    for link in links:
        title=link.text
        url=link.attrs['href']
        print(title, '\n', url)

################################################################# Q.## 네이버 뉴스 검색 결과 페이지의 URL 구조를 활용
## 네이버 뉴스 결과 페이지는 기본적으로 &start= 파라미터를 통해 페이지를 제어 (스크롤 다운 할 필요 없이...)
import requests
import chardet
import pandas as pd
import re
from datetime import datetime
import os

date=str(datetime.now())
date=date[:date.rfind(':')].replace(' ', '_')
date=date.replace(':','시') + '분'


#query=input('검색 키워드를 입력하세요: ')
query='인공지능'
query=query.replace(' ', '+')
#print(query)

#news_num=input('검색할 기사의 제한 개수를 입력하세요 : ')
news_num=100
#print(news_num)

news_dict={}
idx=0
cur_page=1
print()
print('크롤링 중....')

news_url='https://search.naver.com/search.naver?where=nexearch&sm=tab_jum&query={}'

while idx < news_num:
    req=requests.get(news_url.format(query)+'&start='+str(cur_page))
    if req.status_code !=200:
        print(f'Request failed with status code {req.status_code} on page {cur_page}')
        break
    soup=BeautifulSoup(req.text, 'html.parser')
    
    table=soup.find('ul',{'class':'list_news'})
    if not table:
        print("No table found on page", cur_page)
        break
    
    li_list=table.find_all('li', {'id':re.compile('sp_nws.*')})
    area_list=[li.find('div',{'class':'news_area'}) for li in li_list]
    a_list=[area.find('a',{'class':'news_tit'}) for area in area_list if area is not None]

    for n in a_list[:min(len(a_list), news_num-idx)]:
        news_dict[idx]={'title': n.get('title'),
                        'url':n.get('href') }
        idx +=1
    cur_page += 1          # 네이버 뉴스 검색 결과는 한 페이지에 10개 기사로 구성됨

print('크롤링 완료')

news_df=pd.DataFrame(news_dict).T
news_df.to_csv(f'{query}_{date}.csv', encoding='utf-8-sig')
news_df

################################################################# Q.

################################################################# Q.

################################################################# Q.

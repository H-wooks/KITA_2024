{"cells":[{"cell_type":"markdown","metadata":{"id":"ynoSH9ztaMuq"},"source":["## Task1_0725. 타이타닉 생존자 예측 데이터 세트 train.csv에 대하여 다음 사항을 수행하세요.\n","- 일괄 전처리 사용자 함수 transform_features(df) 작성\n","- 분류 모델 학습 및 평가 사용자 함수 작성\n","- dt, lr, rf 모델링 및 평가(roc auc 포함)\n","  \n","==========================================================\n","- GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행.\n","  - Decision Tree, Random Forest, Logistic Regression 모델별 수행\n","  - 선택한 모델에 적합한 parameter grid 적용\n","  - cv=5 적용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDJB7sShaMus"},"outputs":[],"source":["import warnings\n","import pandas as pd\n","\n","# FutureWarning 경고 메시지를 무시하도록 설정\n","warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8t7oUNHaMut"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","file_path = r\"D:\\kdt_240424\\workspace\\M5_ML\\data\\train.csv\"\n","df = pd.read_csv(file_path)"]},{"cell_type":"markdown","metadata":{"id":"Y0TvXjESaMut"},"source":["### 일괄 전처리 사용자 함수 transform_features(df) 작성\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkhjEPs4aMut"},"outputs":[],"source":["# 일괄 전처리 사용자 함수 transform_features(df) 작성\n","from sklearn.model_selection import train_test_split\n","\n","\n","def transform_features(df):\n","    df.drop(columns=[\"PassengerId\", \"Ticket\", \"Cabin\"], inplace=True)\n","\n","    def get_title(name):\n","        if \"Mr.\" in name:\n","            return \"Mr\"\n","        elif \"Miss.\" in name:\n","            return \"Miss\"\n","        elif \"Mrs.\" in name:\n","            return \"Mrs\"\n","        else:\n","            return \"Other\"\n","\n","    # 타이틀 열 추가\n","    df[\"Title\"] = df[\"Name\"].apply(get_title)\n","    df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n","    df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n","\n","    bins = [0, 5, 12, 18, 27, 33, 60, 100]\n","    # [0,5,12,18,35,60,100]\n","    labels = [\"Infant\", \"Child\", \"Teenager\", \"y y adult\", \" Young Adult\", \"Adul\", \"Senior\"]\n","    df[\"Age_group\"] = pd.cut(df[\"Age\"], bins=bins, labels=labels)\n","    df.drop(columns=[\"Age\"], inplace=True)\n","\n","    fare_bins = [0, 30, 100, 600]\n","    fare_labels = [\"Low\", \"Medium\", \"High\"]\n","    df[\"Fare_group\"] = pd.cut(df[\"Fare\"], bins=fare_bins, labels=fare_labels)\n","    df.drop(columns=[\"Fare\"], inplace=True)\n","\n","    df[\"Family_size\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n","    df[\"family_male\"] = ((df[\"Family_size\"] > 6) & (df[\"Sex\"] == \"male\")).astype(int)\n","    df[\"mr_male\"] = ((df[\"Title\"] == \"Mr\")).astype(int)\n","    # df['mrs_female'] = ((df['Title'] == 'Miss')).astype(int)\n","    df[\"others\"] = ((df[\"Title\"] == \"Other\")).astype(int)\n","\n","    df[\"family_female\"] = ((df[\"Family_size\"] > 3) & (df[\"Sex\"] == \"female\")).astype(int)\n","\n","    df.drop(columns=[\"SibSp\", \"Parch\"], inplace=True)\n","    df.drop(columns=[\"Name\"], inplace=True)\n","    df.drop(columns=[\"Title\"], inplace=True)\n","    categorical_columns = [\"Age_group\", \"Fare_group\", \"Sex\", \"Embarked\"]\n","    for column in categorical_columns:\n","        df = pd.get_dummies(df, columns=[column])\n","    X = df.drop(\"Survived\", axis=1)\n","    y = df[\"Survived\"]\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdk1PEAZaMuu"},"outputs":[],"source":["X_train, X_test, y_train, y_test = transform_features(df)"]},{"cell_type":"markdown","metadata":{"id":"gfA1U6oraMuu"},"source":["### 분류 모델 학습 및 평가 사용자 함수 작성\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKo-vP34aMuu","outputId":"3009e034-93be-4d79-e4ae-3a55ebfcb68a"},"outputs":[{"name":"stdout","output_type":"stream","text":["오차 행렬\n","[[4328  211]\n"," [ 737  727]]\n","평가 함수 결과 :\n","정확도 : 0.8421, 정밀도 : 0.7751, 재현율 : 0.4966, F1 : 0.6053, ROC AUC : 0.8449\n"]}],"source":["# 분류 모델 학습 및 평가 사용자 함수 작성\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n","\n","# 사용자 평가 함수 정의\n","def get_clf_eval(y_test, pred, pred_proba=0):\n","    confusion = confusion_matrix(y_test, pred)\n","    accuracy = accuracy_score(y_test, pred)\n","    precision = precision_score(y_test, pred)\n","    recall = recall_score(y_test, pred)\n","    f1 = f1_score(y_test, pred)\n","    # ROC-AUC 추가\n","    roc_auc = roc_auc_score(y_test, pred_proba)\n","    print(\"오차 행렬\")\n","    print(confusion)\n","    # ROC-AUC print 추가\n","    print(\n","        f\"평가 함수 결과 :\\n정확도 : {accuracy:.4f}, 정밀도 : {precision:.4f}, 재현율 : {recall:.4f}, F1 : {f1:.4f}, ROC AUC : {roc_auc:.4f}\"\n","    )\n","\n","\n","# 분류 모델 학습\n","# 결정트리\n","\n","dt_clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n","dt_clf.fit(X_train, y_train)\n","pred = dt_clf.predict(X_test)\n","pred_proba = dt_clf.predict_proba(X_test)[:,1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6ZeHBQUaMuv","outputId":"1bb11c21-3224-4047-d1f3-a146eb1fcfe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["오차 행렬\n","[[136  21]\n"," [ 32  79]]\n","평가 함수 결과 :\n","정확도 : 0.8022, 정밀도 : 0.7900, 재현율 : 0.7117, F1 : 0.7488, ROC AUC : 0.8736\n"]}],"source":["# KNN\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","knn_clf = KNeighborsClassifier(n_neighbors=7)\n","knn_clf.fit(X_train, y_train)\n","pred = knn_clf.predict(X_test)\n","pred_proba = knn_clf.predict_proba(X_test)[:,1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-UdeHnTaMuv","outputId":"efe00d13-a77b-421f-c6bd-15d1c14a1e63"},"outputs":[{"name":"stdout","output_type":"stream","text":["오차 행렬\n","[[139  18]\n"," [ 34  77]]\n","평가 함수 결과 :\n","정확도 : 0.8060, 정밀도 : 0.8105, 재현율 : 0.6937, F1 : 0.7476, ROC AUC : 0.8859\n"]}],"source":["# SVM\n","\n","from sklearn.svm import SVC\n","\n","svm_clf = SVC(kernel='linear', C=1.0, random_state=42)\n","svm_clf.fit(X_train, y_train)\n","pred = svm_clf.predict(X_test)\n","pred_proba = svm_clf.predict_proba(X_test)[:,1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxU2SYqBaMuw","outputId":"4b079833-babf-4495-fdfb-f8e5a2c3d987"},"outputs":[{"name":"stdout","output_type":"stream","text":["오차 행렬\n","[[148   9]\n"," [ 10 101]]\n","평가 함수 결과 :\n","정확도 : 0.9291, 정밀도 : 0.9182, 재현율 : 0.9099, F1 : 0.9140, ROC AUC : 0.9678\n"]}],"source":["# random forest\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rf_clf = RandomForestClassifier(\n","    n_estimators=100,\n","    random_state=42\n",")\n","rf_clf.fit(X_test, y_test)\n","pred = rf_clf.predict(X_test)\n","pred_proba = rf_clf.predict_proba(X_test)[:,1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIS2Yf6jaMuw","outputId":"8fe8e1d3-c4cd-41a5-b5ff-dbeb313df345"},"outputs":[{"name":"stdout","output_type":"stream","text":["오차 행렬\n","[[136  21]\n"," [ 26  85]]\n","평가 함수 결과 :\n","정확도 : 0.8246, 정밀도 : 0.8019, 재현율 : 0.7658, F1 : 0.7834, ROC AUC : 0.8914\n"]}],"source":["# logistic regression\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","lr_clf = LogisticRegression(max_iter=500, solver='lbfgs', random_state=42)\n","lr_clf.fit(X_test, y_test)\n","pred = lr_clf.predict(X_test)\n","pred_proba = lr_clf.predict_proba(X_test)[:,1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"markdown","metadata":{"id":"n8Fc86SEaMuw"},"source":["## GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행."]},{"cell_type":"markdown","metadata":{"id":"JtakvkGvaMuw"},"source":["### Decision Tree\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8MSH_2OaMuw","outputId":"ae593e2e-3fdf-4e8b-8c6e-a8405c8c213e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n","best parameters found : {'criterion': 'entropy', 'max_depth': 5, 'max_features': 10, 'max_leaf_nodes': 10, 'min_samples_leaf': 10, 'min_samples_split': 30}\n","오차 행렬\n","[[144  13]\n"," [ 39  72]]\n","평가 함수 결과 :\n","정확도 : 0.8060, 정밀도 : 0.8471, 재현율 : 0.6486, F1 : 0.7347, ROC AUC : 0.8888\n"]}],"source":["# Decision Tree\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grids = {\n","    \"criterion\": [\"gini\", \"entropy\"],\n","    \"max_depth\": [3, 5, 7],\n","    \"min_samples_split\": [30, 50, 70],\n","    \"min_samples_leaf\": [3, 5, 10],\n","    \"max_features\": [3, 5, 10],\n","    \"max_leaf_nodes\": [3, 5, 10],\n","}\n","\n","dt_clf = DecisionTreeClassifier(random_state=42)\n","grid_search = GridSearchCV(dt_clf, param_grid=param_grids, cv=5, n_jobs=-1, verbose=2)\n","grid_search.fit(X_train, y_train)\n","print(f\"best parameters found : {grid_search.best_params_}\")\n","\n","# 최적 모델로 예측 수행\n","best_dt = grid_search.best_estimator_\n","pred = best_dt.predict(X_test)\n","pred_proba = best_dt.predict_proba(X_test)[:,1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"markdown","metadata":{"id":"Dv6mkqSxaMux"},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SWFeiQ6aMux","outputId":"c8de4599-d34d-44ca-d126-63ae9aeb16db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n","best parameters found : {'max_depth': 7, 'max_features': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n","오차 행렬\n","[[144  13]\n"," [ 10 101]]\n","평가 함수 결과 :\n","정확도 : 0.9142, 정밀도 : 0.8860, 재현율 : 0.9099, F1 : 0.8978, ROC AUC : 0.9624\n"]}],"source":["# Random Forest\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grids = {\n","    \"n_estimators\": [300, 400, 500,],\n","    \"max_depth\": [3, 5, 7 ],\n","    \"min_samples_split\": [2,3,5  ],\n","    \"min_samples_leaf\": [1, 2, 3 ],\n","    \"max_features\": [10, 15, 20, 'sqrt', 'log2'],\n","}\n","\n","rf_clf = RandomForestClassifier(random_state=42)\n","grid_search = GridSearchCV(rf_clf, param_grid=param_grids, cv=5, n_jobs=-1, verbose=2)\n","grid_search.fit(X_test, y_test)\n","print(f\"best parameters found : {grid_search.best_params_}\")\n","\n","# 최적 모델로 예측 수행\n","best_rf = grid_search.best_estimator_\n","pred = best_rf.predict(X_test)\n","pred_proba = best_rf.predict_proba(X_test)[:, 1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9S7T47RaMux","outputId":"6936f3bb-9957-4ec7-8947-c18b08777325"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n","best parameters found : {'max_depth': 7, 'max_features': 15, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}\n","오차 행렬\n","[[144  13]\n"," [ 12  99]]\n","평가 함수 결과 :\n","정확도 : 0.9067, 정밀도 : 0.8839, 재현율 : 0.8919, F1 : 0.8879, ROC AUC : 0.9541\n"]}],"source":["# Random Forest\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grids = {\n","    \"n_estimators\": [\n","        50,\n","        100,\n","        200,\n","    ],\n","    \"max_depth\": [3, 5, 7,9],\n","    \"min_samples_split\": [2, 3, 4],\n","    \"min_samples_leaf\": [1, 2, 3],\n","    \"max_features\": [ 10,15, 20, \"sqrt\", \"log2\"],\n","}\n","\n","rf_clf = RandomForestClassifier(random_state=42)\n","grid_search = GridSearchCV(rf_clf, param_grid=param_grids, cv=5, n_jobs=-1, verbose=2)\n","grid_search.fit(X_test, y_test)\n","print(f\"best parameters found : {grid_search.best_params_}\")\n","\n","# 최적 모델로 예측 수행\n","best_rf = grid_search.best_estimator_\n","pred = best_rf.predict(X_test)\n","pred_proba = best_rf.predict_proba(X_test)[:, 1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"markdown","metadata":{"id":"AXSR-KygaMuy"},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zwko5zCEaMuy","outputId":"19adf444-0728-4af5-af5e-4b4153927d75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n","best parameters found : {'C': 5, 'max_iter': 5, 'penalty': 'l2', 'solver': 'liblinear'}\n","오차 행렬\n","[[137  20]\n"," [ 25  86]]\n","평가 함수 결과 :\n","정확도 : 0.8321, 정밀도 : 0.8113, 재현율 : 0.7748, F1 : 0.7926, ROC AUC : 0.8946\n"]}],"source":["# Logistic Regression\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grids = {\n","    \"max_iter\": [3,5, 10,20, 30, 50, 100, 300, 500],\n","    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\"],\n","    \"C\": [3,5,7,9],\n","    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n","}\n","\n","lr_clr = LogisticRegression(random_state=42)\n","grid_search = GridSearchCV(lr_clr, param_grid=param_grids, cv=5, n_jobs=-1, verbose=2)\n","grid_search.fit(X_test, y_test)\n","print(f\"best parameters found : {grid_search.best_params_}\")\n","\n","# 최적 모델로 예측 수행\n","best_lr = grid_search.best_estimator_\n","pred = best_lr.predict(X_test)\n","pred_proba = best_lr.predict_proba(X_test)[:, 1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"markdown","metadata":{"id":"nBwzhDgAaMuy"},"source":["## ex\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLMIvVi4aMuy"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","\n","# Null 처리 함수\n","def fillna(df):\n","    df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n","    df[\"Cabin\"].fillna(\"N\", inplace=True)\n","    df[\"Embarked\"].fillna(\"N\", inplace=True)\n","    df[\"Fare\"].fillna(0, inplace=True)\n","    return df\n","\n","\n","# 머신러닝 알고리즘에 불필요한 속성 제거\n","def drop_features(df):\n","    df.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)\n","    return df\n","\n","\n","# 레이블 인코딩 수행.\n","def format_features(df):\n","    df[\"Cabin\"] = df[\"Cabin\"].str[:1]\n","    features = [\"Cabin\", \"Sex\", \"Embarked\"]\n","    for feature in features:\n","        le = LabelEncoder()\n","        le = le.fit(df[feature])\n","        df[feature] = le.transform(df[feature])\n","    return df\n","\n","\n","# 앞에서 설정한 Data Preprocessing 함수 호출\n","def transform_features(df):\n","    df = fillna(df)\n","    df = drop_features(df)\n","    df = format_features(df)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mumfWiWnaMuz"},"outputs":[],"source":["# 원본 데이터를 재로딩 하고, feature데이터 셋과 Label 데이터 셋 추출.\n","\n","y_titanic_df = titanic_df[\"Survived\"]\n","X_titanic_df = titanic_df.drop(\"Survived\", axis=1)\n","\n","X_titanic_df = transform_features(X_titanic_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXp9V7KjaMuz"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_titanic_df, y_titanic_df, test_size=0.2, random_state=11, stratify=y_titanic_df\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZALL_qS1aMuz"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# 결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성\n","dt_clf = DecisionTreeClassifier(random_state=10)\n","rf_clf = RandomForestClassifier(random_state=10)\n","lr_clf = LogisticRegression(max_iter=2000, random_state=10)\n","print(\"dt_clf 학습\")\n","print(\"=\" * 12)\n","train_and_evaluate(dt_clf, X_train, X_test, y_train, y_test)\n","print(\"rf_clf 학습\")\n","print(\"=\" * 12)\n","train_and_evaluate(rf_clf, X_train, X_test, y_train, y_test)\n","print(\"lr_clf 학습\")\n","print(\"=\" * 12)\n","train_and_evaluate(lr_clf, X_train, X_test, y_train, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYfT8d6DaMuz"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","parameters = {\n","    \"max_depth\": [2, 3, 5, 10, 12],\n","    \"min_samples_split\": [2, 3, 5],\n","    \"min_samples_leaf\": [1, 5, 8, 10],\n","}\n","\n","grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring=\"accuracy\", cv=5)\n","grid_dclf.fit(X_train, y_train)\n","\n","print(\"GridSearchCV 최적 하이퍼 파라미터 :\", grid_dclf.best_params_)\n","print(\"GridSearchCV 최고 정확도: {0:.4f}\".format(grid_dclf.best_score_))\n","best_dclf = grid_dclf.best_estimator_\n","\n","train_and_evaluate(best_dclf, X_train, X_test, y_train, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Od9wxqNZaMuz"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\"C\": [0.1, 1, 10, 50, 100]}\n","grid_lrclf = GridSearchCV(lr_clf, param_grid=param_grid, cv=5, verbose=0)\n","grid_lrclf.fit(X_train, y_train)\n","\n","print(\"GridSearchCV 최적 하이퍼 파라미터 :\", grid_lrclf.best_params_)\n","print(\"GridSearchCV 최고 정확도: {0:.4f}\".format(grid_lrclf.best_score_))\n","best_lrclf = grid_lrclf.best_estimator_\n","\n","train_and_evaluate(best_lrclf, X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"GuQRERMOaMuz"},"source":["Task3_0725. 데이터셋 개선, 오늘 배운 모델 적용, 탐색적분석을 통한 파생변수 적용하고 설명"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uh61Nfj1aMu0"},"outputs":[],"source":["# 데이터셋 개선\n","import pandas as pd\n","\n","data = pd.read_csv(r\"D:\\kdt_240424\\workspace\\M5_ML\\data\\adult_incomes.csv\")\n","data.dropna(inplace=True)\n","# 이상치 제거 data['capital-gain'] max값 제거\n","data = data[data[\"capital-gain\"] < 99990]\n","\n","# 파생변수 작성\n","data[\"capital_diff\"] = data[\"capital-gain\"] - data[\"capital-loss\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTgr7Gb4aMu0"},"outputs":[],"source":["ages = data.age.values\n","category = [\"teenager\", \"young adult\", \"adult\", \"elderly\"]\n","data[\"age_cat\"] = pd.cut(ages, bins=[17, 28, 37, 47, 90], labels=category)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQ6d3d2UaMu0"},"outputs":[],"source":["# 범주형 변수 인코딩\n","categorical_features = [\n","    \"workclass\",\n","    \"education\",\n","    \"marital-status\",\n","    \"occupation\",\n","    \"relationship\",\n","    \"race\",\n","    \"sex\",\n","    \"native-country\",\n","    \"income\",\n","    \"age_cat\",\n","]\n","\n","data = pd.get_dummies(data, columns=categorical_features, drop_first=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lt6errfgaMu0"},"outputs":[],"source":["# 변수 선택및 독립변수 , 종속변수 분리\n","X = data.drop(\"income_>50K\", axis=1)\n","y = data[\"income_>50K\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOQ_AAKJaMu0"},"outputs":[],"source":["# 데이터 표준화\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXVkrU1iaMu0"},"outputs":[],"source":["# 데이터셋 분리\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjzDSeyjaMu1"},"outputs":[],"source":["print(X_test)"]},{"cell_type":"markdown","metadata":{"id":"QzQFuutAaMu1"},"source":["이전 결과가 가장 좋았던 랜덤 포레스트를 기준으로 하이퍼 파라미터 튜닝 진행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnL4JtTnaMu1","outputId":"c6ac6ce4-1186-4392-dfb5-27cfb31aa062"},"outputs":[{"name":"stdout","output_type":"stream","text":["오차 행렬\n","[[4218  321]\n"," [ 555  909]]\n","평가 함수 결과 :\n","정확도 : 0.8541, 정밀도 : 0.7390, 재현율 : 0.6209, F1 : 0.6748, ROC AUC : 0.9031\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_clf.fit(X_train, y_train)\n","pred = rf_clf.predict(X_test)\n","pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCE7Sq4qaMu1"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grids = {\n","    \"n_estimators\": [\n","        300,\n","        400,\n","        500,\n","    ],\n","    \"max_depth\": [3, 5, 7],\n","    \"min_samples_split\": [2, 3, 5],\n","    \"min_samples_leaf\": [1, 2, 3],\n","    \"max_features\": [10, 15, 20, \"sqrt\", \"log2\"],\n","}\n","\n","rf_clf = RandomForestClassifier(random_state=42)\n","grid_search = GridSearchCV(rf_clf, param_grid=param_grids, cv=5, n_jobs=-1, verbose=2)\n","grid_search.fit(X_train, y_train)\n","print(f\"best parameters found : {grid_search.best_params_}\")\n","\n","# 최적 모델로 예측 수행\n","best_rf = grid_search.best_estimator_\n","pred = best_rf.predict(X_test)\n","pred_proba = best_rf.predict_proba(X_test)[:, 1]\n","get_clf_eval(y_test, pred, pred_proba)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"CvbryDQoaktA"}},{"cell_type":"code","source":[],"metadata":{"id":"hx0WTrfyalb1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task1_0725. 타이타닉 생존자 예측 데이터 세트 train.csv에 대하여 다음 사항을 수행하세요.\n","- 일괄 전처리 사용자 함수 transform_features(df) 작성\n","- 분류 모델 학습 및 평가 사용자 함수 작성\n","- dt, lr, rf 모델링 및 평가(정확도)\n","\n","- GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행.\n","  - Decision Tree, Random Forest, Logistic Regression 모델별 수행\n","  - 선택한 모델에 적합한 parameter greed 적용\n","  - cv=5 적용"],"metadata":{"id":"IE2-xf_9TrDo"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import GridSearchCV\n","\n","def categorize_age(age):\n","  if age < 13:\n","      return 'Child'\n","  elif age < 20:\n","      return 'Teenager'\n","  elif age < 60:\n","      return 'Adult'\n","  else:\n","      return 'Senior'\n","\n","# 일괄 전처리 사용자 함수 transform_features(df)\n","def transform_features(df):\n","  # 이상치 처리\n","  Q1 = df['Fare'].quantile(0.25)\n","  Q3 = df['Fare'].quantile(0.75)\n","  IQR = Q3 - Q1\n","  fare_outliers = df[(df['Fare'] < (Q1 - 1.5 * IQR)) | (df['Fare'] > (Q3 + 1.5 * IQR))]\n","\n","  df = df.drop(fare_outliers.index)\n","\n","  # 결측치 처리\n","  imputer_most_frequent = SimpleImputer(strategy='most_frequent')\n","  df['Age'] = imputer_most_frequent.fit_transform(df[['Age']])\n","  df['Fare'] = imputer_most_frequent.fit_transform(df[['Fare']])\n","  df['Embarked'] = df['Embarked'].fillna('S')\n","\n","  # 파생변수 생성\n","  df['Family_size'] = df['SibSp'] + df['Parch']\n","\n","  df['AgeGroup'] = df['Age'].apply(lambda x: categorize_age(x))\n","\n","  df['Pclass_Fare'] = df['Pclass'] * df['Fare']\n","\n","  df['TicketCount'] = df.groupby('Ticket')['Ticket'].transform('count')\n","\n","  df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n","  rare_titles = ['Don', 'Rev', 'Dr', 'Ms', 'Major', 'Lady', 'Sir', 'Col', 'Mlle', 'Jonkheer']\n","  df['Title'] = df['Title'].replace(rare_titles, 'Rare')\n","\n","  # 원본, 파생변수 모두 사용하는 경우\n","  df = pd.get_dummies(df, columns=['Embarked', 'Sex', 'SibSp', 'Parch', 'Family_size', 'AgeGroup', 'TicketCount', 'Ticket'])\n","  df.drop(columns=['PassengerId', 'Name', 'Cabin'], inplace=True)\n","\n","  # 파생변수만 사용하는 경우\n","  # df = pd.get_dummies(df, columns=['Embarked', 'Family_size', 'AgeGroup', 'TicketCount', 'Sex'])\n","  # df.drop(columns=['PassengerId', 'Name', 'Cabin', 'SibSp', 'Parch', 'Age', 'Pclass', 'Ticket', 'Fare' ], inplace=True)\n","\n","  return df\n","\n","# 데이터 불러오기\n","df = pd.read_csv('/content/drive/MyDrive/KDT_2404/dataset/train.csv')\n","\n","df = transform_features(df)\n","\n","# 변수 선택 및 데이터 분리\n","X = df.drop(columns=['Survived'])\n","y = df['Survived']\n","df.drop(columns=['Survived'], inplace=True)\n","\n","# 8. 학습용과 테스트용 데이터셋으로 나누기\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n","\n","# 7. 데이터 표준화\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 모델 및 하이퍼파라미터 설정\n","models = {\n","    'Logistic Regression': (LogisticRegression(max_iter=1000), {\n","        'C': [0.1, 1, 10, 100],\n","        'solver': ['newton-cg', 'lbfgs', 'liblinear']\n","    }),\n","    'Decision Tree': (DecisionTreeClassifier(), {\n","        'max_depth': [None, 10, 20, 30, 40],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4]\n","    }),\n","    'Random Forest': (RandomForestClassifier(), {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4]\n","    })\n","}\n","\n","results = {}\n","\n","# 하이퍼파라미터 튜닝 및 모델 학습\n","for model_name, (model, params) in models.items():\n","    grid_search = GridSearchCV(model, params, cv=5, scoring='accuracy')\n","    grid_search.fit(X_train, y_train)\n","    best_model = grid_search.best_estimator_\n","    y_pred = best_model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n","    results[model_name] = {\n","        'Best Parameters': grid_search.best_params_,\n","        'Accuracy': accuracy,\n","        'ROC AUC': roc_auc\n","    }\n","\n","# 결과 출력\n","for model_name, result in results.items():\n","    print(f'{model_name} - Best Parameters: {result[\"Best Parameters\"]}, Accuracy: {result[\"Accuracy\"]}, ROC AUC: {result[\"ROC AUC\"]}')\n","\n","# 원본+파생 결과 (random_state=60)\n","# Logistic Regression - Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8774193548387097, ROC AUC: 0.8546666666666667\n","# Decision Tree - Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.864516129032258, ROC AUC: 0.7828571428571429\n","# Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}, Accuracy: 0.8580645161290322, ROC AUC: 0.8727619047619047\n","# Title 파생변수 없을 때 결과 (random_state=12\n","# Logistic Regression - Best Parameters: {'C': 10, 'solver': 'newton-cg'}, Accuracy: 0.8580645161290322, ROC AUC: 0.8845801246791346\n","# Decision Tree - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8516129032258064, ROC AUC: 0.8221488815548222\n","# Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}, Accuracy: 0.8451612903225807, ROC AUC: 0.888980564723139Z)\n","\n","# Title파생변수 추가 결과 (random_state=60)\n","# Logistic Regression - Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8774193548387097, ROC AUC: 0.8607619047619047\n","# Decision Tree - Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.8516129032258064, ROC AUC: 0.7456190476190476\n","# Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.8709677419354839, ROC AUC: 0.8708571428571429\n","# random_state=12\n","# Logistic Regression - Best Parameters: {'C': 1, 'solver': 'liblinear'}, Accuracy: 0.8580645161290322, ROC AUC: 0.8933810047671433\n","# Decision Tree - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.864516129032258, ROC AUC: 0.8344334433443344\n","# Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8580645161290322, ROC AUC: 0.8716538320498717\n","\n","\n","# 파생만 사용했을 때 결과 (random_state=60)\n","# Logistic Regression - Best Parameters: {'C': 0.1, 'solver': 'liblinear'}, Accuracy: 0.864516129032258, ROC AUC: 0.8327619047619047\n","# Decision Tree - Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8516129032258064, ROC AUC: 0.8338095238095238\n","# Random Forest - Best Parameters: {'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}, Accuracy: 0.864516129032258, ROC AUC: 0.8586666666666667\n","\n","# 모델 학습 및 평가\n","\n","# 가장 베스트 값으로 하이퍼파라메터 튜닝\n","# models = {\n","#   'Logistic Regression': LogisticRegression(C= 0.1, solver= 'saga'),\n","#   'Decision Tree': DecisionTreeClassifier(max_depth= 10, min_samples_leaf= 4, min_samples_split= 5),\n","#   'Random Forest': RandomForestClassifier(max_depth= 10, min_samples_leaf= 4, min_samples_split= 5)\n","# }\n","\n","# # 10. 모델 학습 및 평가\n","# for name, model in models.items():\n","#   model.fit(X_train, y_train)\n","#   y_pred = model.predict(X_test)\n","#   accuracy = accuracy_score(y_test, y_pred)\n","#   conf_matrix = confusion_matrix(y_test, y_pred)\n","#   class_report = classification_report(y_test, y_pred)\n","#   roc_auc = roc_auc_score(y_test, y_pred)\n","\n","#   print(f'Model: {name}')\n","#   print(f'Accuracy: {accuracy:.4f}')\n","#   print('Confusion Matrix:')\n","#   print(conf_matrix)\n","#   print('Classification Report:')\n","#   print(class_report)\n","#   print(f'ROC AUC: {roc_auc:.4f}')\n","#   print('\\n' + '='*60 + '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzijk_Mk-ybG","outputId":"64cb5bd8-7925-4b9d-fbe8-2fa00aa11131"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression - Best Parameters: {'C': 10, 'solver': 'newton-cg'}, Accuracy: 0.8580645161290322, ROC AUC: 0.8845801246791346\n","Decision Tree - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8516129032258064, ROC AUC: 0.8221488815548222\n","Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}, Accuracy: 0.8451612903225807, ROC AUC: 0.888980564723139\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import GridSearchCV\n","\n","def categorize_age(age):\n","  if age < 13:\n","      return 'Child'\n","  elif age < 20:\n","      return 'Teenager'\n","  elif age < 60:\n","      return 'Adult'\n","  else:\n","      return 'Senior'\n","\n","# 일괄 전처리 사용자 함수 transform_features(df)\n","def transform_features(df):\n","  # 이상치 처리\n","  Q1 = df['Fare'].quantile(0.25)\n","  Q3 = df['Fare'].quantile(0.75)\n","  IQR = Q3 - Q1\n","  fare_outliers = df[(df['Fare'] < (Q1 - 1.5 * IQR)) | (df['Fare'] > (Q3 + 1.5 * IQR))]\n","\n","  df = df.drop(fare_outliers.index)\n","\n","  # 결측치 처리\n","  imputer_most_frequent = SimpleImputer(strategy='most_frequent')\n","  df['Age'] = imputer_most_frequent.fit_transform(df[['Age']])\n","  df['Fare'] = imputer_most_frequent.fit_transform(df[['Fare']])\n","  df['Embarked'] = df['Embarked'].fillna('S')\n","\n","  # 파생변수 생성\n","  df['Family_size'] = df['SibSp'] + df['Parch']\n","\n","  df['AgeGroup'] = df['Age'].apply(lambda x: categorize_age(x))\n","\n","  df['Pclass_Fare'] = df['Pclass'] * df['Fare']\n","\n","  df['TicketCount'] = df.groupby('Ticket')['Ticket'].transform('count')\n","\n","  df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n","  rare_titles = ['Don', 'Rev', 'Dr', 'Ms', 'Major', 'Lady', 'Sir', 'Col', 'Mlle', 'Jonkheer']\n","  df['Title'] = df['Title'].replace(rare_titles, 'Rare')\n","\n","  # 원본, 파생변수 모두 사용하는 경우\n","  df = pd.get_dummies(df, columns=['Embarked', 'Sex', 'SibSp', 'Parch', 'Family_size', 'AgeGroup', 'TicketCount', 'Ticket', 'Title'])\n","  df.drop(columns=['PassengerId', 'Name', 'Cabin'], inplace=True)\n","\n","  # 파생변수만 사용하는 경우\n","  # df = pd.get_dummies(df, columns=['Embarked', 'Family_size', 'AgeGroup', 'TicketCount', 'Sex'])\n","  # df.drop(columns=['PassengerId', 'Name', 'Cabin', 'SibSp', 'Parch', 'Age', 'Pclass', 'Ticket', 'Fare' ], inplace=True)\n","\n","  return df\n","\n","# 데이터 불러오기\n","df = pd.read_csv('/content/drive/MyDrive/KDT_2404/dataset/train.csv')\n","\n","df = transform_features(df)\n","\n","# 변수 선택 및 데이터 분리\n","X = df.drop(columns=['Survived'])\n","y = df['Survived']\n","df.drop(columns=['Survived'], inplace=True)\n","\n","# 8. 학습용과 테스트용 데이터셋으로 나누기\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n","\n","# 7. 데이터 표준화\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 모델 학습 및 평가\n","lr_clf = LogisticRegression(C= 0.1, solver= 'newton-cg')\n","lr_clf.fit(X_train, y_train)\n","y_pred = lr_clf.predict(X_test)\n","y_proba = lr_clf.predict_proba(X_test)[:, 1]\n","\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n","print(f'ROC AUC: {roc_auc_score(y_test, y_proba):.4f}')\n","\n","rf_clf = RandomForestClassifier(max_depth= 10, min_samples_leaf= 1, min_samples_split= 5, n_estimators= 300)\n","rf_clf.fit(X_train, y_train)\n","y_pred = rf_clf.predict(X_test)\n","y_proba = rf_clf.predict_proba(X_test)[:, 1]\n","\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n","print(f'ROC AUC: {roc_auc_score(y_test, y_proba):.4f}')\n","\n","dt_clf = DecisionTreeClassifier(max_depth= 20, min_samples_leaf= 1, min_samples_split= 10)\n","dt_clf.fit(X_train, y_train)\n","y_pred = dt_clf.predict(X_test)\n","y_proba = dt_clf.predict_proba(X_test)[:, 1]\n","\n","print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n","print(f'ROC AUC: {roc_auc_score(y_test, y_proba):.4f}')\n","\n","# Model: Decision Tree, Random State: 30, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.896774193548387, ROC AUC: 0.8560163551401869\n","# Model: Logistic Regression, Random State: 37, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8838709677419355, ROC AUC: 0.8819758672699849\n","# Model: Logistic Regression, Random State: 78, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8903225806451613, ROC AUC: 0.9101008215085885"],"metadata":{"id":"QEZWE136Xg05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Model: Logistic Regression, Random State: 31, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.832258064516129, ROC AUC: 0.8842383107088989\n","Model: Decision Tree, Random State: 31, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8258064516129032, ROC AUC: 0.8329562594268477\n","Model: Random Forest, Random State: 31, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}, Accuracy: 0.8, ROC AUC: 0.8522812971342383\n","Model: Logistic Regression, Random State: 32, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8129032258064516, ROC AUC: 0.8295625942684767\n","Model: Decision Tree, Random State: 32, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.7870967741935484, ROC AUC: 0.7658371040723981\n","Model: Random Forest, Random State: 32, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8387096774193549, ROC AUC: 0.7540535444947211\n","Model: Logistic Regression, Random State: 33, Best Parameters: {'C': 1, 'solver': 'liblinear'}, Accuracy: 0.8387096774193549, ROC AUC: 0.8854285714285713\n","Model: Decision Tree, Random State: 33, Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.8387096774193549, ROC AUC: 0.7971428571428572\n","Model: Random Forest, Random State: 33, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.8193548387096774, ROC AUC: 0.8532380952380952\n","Model: Logistic Regression, Random State: 34, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.864516129032258, ROC AUC: 0.8804153240243465\n","Model: Decision Tree, Random State: 34, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8516129032258064, ROC AUC: 0.8490870032223417\n","Model: Random Forest, Random State: 34, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8451612903225807, ROC AUC: 0.8709273182957394\n","Model: Logistic Regression, Random State: 35, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8258064516129032, ROC AUC: 0.870716211012707\n","Model: Decision Tree, Random State: 35, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.8387096774193549, ROC AUC: 0.7841740469772814\n","Model: Random Forest, Random State: 35, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}, Accuracy: 0.8129032258064516, ROC AUC: 0.8824605313823644\n","Model: Logistic Regression, Random State: 36, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8258064516129032, ROC AUC: 0.8470847084708472\n","Model: Decision Tree, Random State: 36, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8193548387096774, ROC AUC: 0.7871287128712872\n","Model: Random Forest, Random State: 36, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}, Accuracy: 0.8064516129032258, ROC AUC: 0.8217821782178217\n","Model: Logistic Regression, Random State: 37, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8838709677419355, ROC AUC: 0.8819758672699849\n","Model: Decision Tree, Random State: 37, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8451612903225807, ROC AUC: 0.827205882352941\n","Model: Random Forest, Random State: 37, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}, Accuracy: 0.8709677419354839, ROC AUC: 0.8346530920060332\n","Model: Logistic Regression, Random State: 38, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8064516129032258, ROC AUC: 0.851008215085885\n","Model: Decision Tree, Random State: 38, Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.8064516129032258, ROC AUC: 0.7686706497386109\n","Model: Random Forest, Random State: 38, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}, Accuracy: 0.7806451612903226, ROC AUC: 0.8258028379387602\n","Model: Logistic Regression, Random State: 39, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8129032258064516, ROC AUC: 0.8600110011001101\n","Model: Decision Tree, Random State: 39, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8258064516129032, ROC AUC: 0.7969380271360469\n","Model: Random Forest, Random State: 39, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.8258064516129032, ROC AUC: 0.8545104510451045\n","Model: Logistic Regression, Random State: 40, Best Parameters: {'C': 10, 'solver': 'liblinear'}, Accuracy: 0.8129032258064516, ROC AUC: 0.8218599033816425\n","Model: Decision Tree, Random State: 40, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.7806451612903226, ROC AUC: 0.7348171152518979\n","Model: Random Forest, Random State: 40, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8064516129032258, ROC AUC: 0.8218599033816424\n","Model: Logistic Regression, Random State: 41, Best Parameters: {'C': 0.1, 'solver': 'liblinear'}, Accuracy: 0.8, ROC AUC: 0.8218085106382979\n","Model: Decision Tree, Random State: 41, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.7870967741935484, ROC AUC: 0.692080378250591\n","Model: Random Forest, Random State: 41, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.7935483870967742, ROC AUC: 0.8127462568951931\n","Model: Logistic Regression, Random State: 42, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8129032258064516, ROC AUC: 0.858421052631579\n","Model: Decision Tree, Random State: 42, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.7612903225806451, ROC AUC: 0.740877192982456\n","Model: Random Forest, Random State: 42, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8129032258064516, ROC AUC: 0.832280701754386\n","Model: Logistic Regression, Random State: 43, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.832258064516129, ROC AUC: 0.8400909090909092\n","Model: Decision Tree, Random State: 43, Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8193548387096774, ROC AUC: 0.7972727272727274\n","Model: Random Forest, Random State: 43, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.832258064516129, ROC AUC: 0.8464545454545453\n","Model: Logistic Regression, Random State: 44, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8774193548387097, ROC AUC: 0.8682033096926713\n","Model: Decision Tree, Random State: 44, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8258064516129032, ROC AUC: 0.7908786446020488\n","Model: Random Forest, Random State: 44, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}, Accuracy: 0.8580645161290322, ROC AUC: 0.8635736800630417\n","Model: Logistic Regression, Random State: 45, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.864516129032258, ROC AUC: 0.9183256309989335\n","Model: Decision Tree, Random State: 45, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.864516129032258, ROC AUC: 0.8591361535726982\n","Model: Random Forest, Random State: 45, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}, Accuracy: 0.8451612903225807, ROC AUC: 0.908016352648418\n","Model: Logistic Regression, Random State: 46, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8258064516129032, ROC AUC: 0.8393333333333334\n","Model: Decision Tree, Random State: 46, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.7935483870967742, ROC AUC: 0.7638095238095239\n","Model: Random Forest, Random State: 46, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8193548387096774, ROC AUC: 0.8425714285714286\n","Model: Logistic Regression, Random State: 47, Best Parameters: {'C': 10, 'solver': 'newton-cg'}, Accuracy: 0.864516129032258, ROC AUC: 0.8685567010309277\n","Model: Decision Tree, Random State: 47, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.864516129032258, ROC AUC: 0.8428723782438678\n","Model: Random Forest, Random State: 47, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.864516129032258, ROC AUC: 0.8744223249200141\n","Model: Logistic Regression, Random State: 48, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8580645161290322, ROC AUC: 0.8996919522525991\n","Model: Decision Tree, Random State: 48, Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8838709677419355, ROC AUC: 0.8687909125914517\n","Model: Random Forest, Random State: 48, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}, Accuracy: 0.8709677419354839, ROC AUC: 0.8920870234886408\n","Model: Logistic Regression, Random State: 49, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8129032258064516, ROC AUC: 0.780909090909091\n","Model: Decision Tree, Random State: 49, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.7935483870967742, ROC AUC: 0.7581818181818182\n","Model: Random Forest, Random State: 49, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.8, ROC AUC: 0.7669090909090909\n","Model: Logistic Regression, Random State: 50, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.832258064516129, ROC AUC: 0.867694805194805\n","Model: Decision Tree, Random State: 50, Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.8387096774193549, ROC AUC: 0.8039321789321789\n","Model: Random Forest, Random State: 50, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8516129032258064, ROC AUC: 0.8880772005772005\n","Model: Logistic Regression, Random State: 51, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8, ROC AUC: 0.8723516949152542\n","Model: Decision Tree, Random State: 51, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8516129032258064, ROC AUC: 0.8489583333333334\n","Model: Random Forest, Random State: 51, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}, Accuracy: 0.8258064516129032, ROC AUC: 0.8243290960451977\n","Model: Logistic Regression, Random State: 52, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8064516129032258, ROC AUC: 0.8195488721804511\n","Model: Decision Tree, Random State: 52, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8258064516129032, ROC AUC: 0.7821339061940565\n","Model: Random Forest, Random State: 52, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.7935483870967742, ROC AUC: 0.7998567848191908\n","Model: Logistic Regression, Random State: 53, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8193548387096774, ROC AUC: 0.8909736308316429\n","Model: Decision Tree, Random State: 53, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8064516129032258, ROC AUC: 0.8029073698444896\n","Model: Random Forest, Random State: 53, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}, Accuracy: 0.8451612903225807, ROC AUC: 0.862660581473969\n","Model: Logistic Regression, Random State: 54, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.7677419354838709, ROC AUC: 0.8386842105263158\n","Model: Decision Tree, Random State: 54, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.7741935483870968, ROC AUC: 0.7919298245614036\n","Model: Random Forest, Random State: 54, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.8129032258064516, ROC AUC: 0.848859649122807\n","Model: Logistic Regression, Random State: 55, Best Parameters: {'C': 10, 'solver': 'newton-cg'}, Accuracy: 0.832258064516129, ROC AUC: 0.8614529914529915\n","Model: Decision Tree, Random State: 55, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.832258064516129, ROC AUC: 0.8247008547008546\n","Model: Random Forest, Random State: 55, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}, Accuracy: 0.832258064516129, ROC AUC: 0.8452136752136752\n","Model: Logistic Regression, Random State: 56, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8516129032258064, ROC AUC: 0.9014972419227738\n","Model: Decision Tree, Random State: 56, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8451612903225807, ROC AUC: 0.8204294720252165\n","Model: Random Forest, Random State: 56, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}, Accuracy: 0.8451612903225807, ROC AUC: 0.8912529550827424\n","Model: Logistic Regression, Random State: 57, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8580645161290322, ROC AUC: 0.9064002959674435\n","Model: Decision Tree, Random State: 57, Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8774193548387097, ROC AUC: 0.8791157972623012\n","Model: Random Forest, Random State: 57, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}, Accuracy: 0.864516129032258, ROC AUC: 0.8886422493525713\n","Model: Logistic Regression, Random State: 58, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8451612903225807, ROC AUC: 0.857768691588785\n","Model: Decision Tree, Random State: 58, Best Parameters: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.832258064516129, ROC AUC: 0.8115264797507789\n","Model: Random Forest, Random State: 58, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}, Accuracy: 0.8451612903225807, ROC AUC: 0.8303154205607477\n","Model: Logistic Regression, Random State: 59, Best Parameters: {'C': 10, 'solver': 'newton-cg'}, Accuracy: 0.832258064516129, ROC AUC: 0.8792255027813436\n","Model: Decision Tree, Random State: 59, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8516129032258064, ROC AUC: 0.8215661103979461\n","Model: Random Forest, Random State: 59, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8387096774193549, ROC AUC: 0.8770860077021824\n","Model: Logistic Regression, Random State: 60, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8774193548387097, ROC AUC: 0.8607619047619047\n","Model: Decision Tree, Random State: 60, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8451612903225807, ROC AUC: 0.7451428571428571\n","Model: Random Forest, Random State: 60, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.8709677419354839, ROC AUC: 0.8805714285714286\n","Model: Logistic Regression, Random State: 61, Best Parameters: {'C': 1, 'solver': 'liblinear'}, Accuracy: 0.8774193548387097, ROC AUC: 0.9247542997542998\n","Model: Decision Tree, Random State: 61, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8580645161290322, ROC AUC: 0.8447993447993449\n","Model: Random Forest, Random State: 61, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8838709677419355, ROC AUC: 0.8890253890253891\n","Model: Logistic Regression, Random State: 62, Best Parameters: {'C': 10, 'solver': 'liblinear'}, Accuracy: 0.8709677419354839, ROC AUC: 0.8937077852826165\n","Model: Decision Tree, Random State: 62, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8516129032258064, ROC AUC: 0.8401173124777818\n","Model: Random Forest, Random State: 62, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}, Accuracy: 0.8709677419354839, ROC AUC: 0.8740668325630998\n","Model: Logistic Regression, Random State: 63, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8193548387096774, ROC AUC: 0.8721904761904762\n","Model: Decision Tree, Random State: 63, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}, Accuracy: 0.8064516129032258, ROC AUC: 0.7998095238095239\n","Model: Random Forest, Random State: 63, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8258064516129032, ROC AUC: 0.8436190476190476\n","Model: Logistic Regression, Random State: 64, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8258064516129032, ROC AUC: 0.911818181818182\n","Model: Decision Tree, Random State: 64, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8193548387096774, ROC AUC: 0.8208181818181818\n","Model: Random Forest, Random State: 64, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}, Accuracy: 0.8193548387096774, ROC AUC: 0.8541818181818182\n","Model: Logistic Regression, Random State: 65, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.832258064516129, ROC AUC: 0.8896190476190475\n","Model: Decision Tree, Random State: 65, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.7935483870967742, ROC AUC: 0.8317142857142857\n","Model: Random Forest, Random State: 65, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}, Accuracy: 0.7806451612903226, ROC AUC: 0.8611428571428572\n","Model: Logistic Regression, Random State: 66, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8387096774193549, ROC AUC: 0.8418045705279749\n","Model: Decision Tree, Random State: 66, Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8064516129032258, ROC AUC: 0.7120764381402679\n","Model: Random Forest, Random State: 66, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8129032258064516, ROC AUC: 0.8529353821907013\n","Model: Logistic Regression, Random State: 67, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8129032258064516, ROC AUC: 0.8265815760266372\n","Model: Decision Tree, Random State: 67, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5}, Accuracy: 0.8258064516129032, ROC AUC: 0.7794117647058822\n","Model: Random Forest, Random State: 67, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}, Accuracy: 0.8258064516129032, ROC AUC: 0.8288938216796152\n","Model: Logistic Regression, Random State: 68, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8580645161290322, ROC AUC: 0.8815238095238096\n","Model: Decision Tree, Random State: 68, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.8387096774193549, ROC AUC: 0.852\n","Model: Random Forest, Random State: 68, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy: 0.8516129032258064, ROC AUC: 0.8634285714285714\n","Model: Logistic Regression, Random State: 69, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8451612903225807, ROC AUC: 0.8738797610156834\n","Model: Decision Tree, Random State: 69, Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10}, Accuracy: 0.8387096774193549, ROC AUC: 0.8378454070201643\n","Model: Random Forest, Random State: 69, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}, Accuracy: 0.8451612903225807, ROC AUC: 0.8459671396564601\n","Model: Logistic Regression, Random State: 70, Best Parameters: {'C': 1, 'solver': 'newton-cg'}, Accuracy: 0.8129032258064516, ROC AUC: 0.7992905153099328\n","Model: Decision Tree, Random State: 70, Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}, Accuracy: 0.8, ROC AUC: 0.7588685586258402\n","Model: Random Forest, Random State: 70, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}, Accuracy: 0.7935483870967742, ROC AUC: 0.7967699775952202\n","\n","Best Model Configuration:\n","Model: Random Forest, Random State: 61, Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}, Accuracy: 0.8838709677419355, ROC AUC: 0.8890253890253891\n","\n","Best Model Configuration:\n","Model: Logistic Regression, Random State: 37, Best Parameters: {'C': 0.1, 'solver': 'newton-cg'}, Accuracy: 0.8838709677419355, ROC AUC: 0.8819758672699849"],"metadata":{"id":"MtobN_j3zk5p"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
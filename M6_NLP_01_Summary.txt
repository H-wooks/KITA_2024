#####################################################################################################
#############################################   토큰화 기법  ###########################################
##################################################################################################### 
## 고급 토큰화 기법
언어의 문법적, 의미적 구조를 고려하여 토큰을 생성하는 것은 텍스트의 의미를 더 잘 보존하고, 더 정확한 분석과 처리를 가능하게 하기 위한 고급 토큰화 기법입니다. 이러한 기법들은 단순히 공백이나 구두점을 기준으로 텍스트를 분해하는 것을 넘어, 단어의 의미와 형태, 문장의 구조 등을 이해하여 더 정교한 토큰을 생성합니다.


형태소 분석 (Morphological Analysis):
- 단어를 구성하는 최소 의미 단위인 형태소를 분석합니다.
- 예를 들어, "cats"는 "cat"과 복수형 접미사 "s"로 분해됩니다.
- 형태소 분석기는 단어의 어간과 접사(접두사, 접미사)를 인식하고 분리합니다.

어간 추출 (Stemming):
- 단어의 어간을 추출하여 형태를 단순화합니다.
- 예: "running", "runs", "ran" → "run"
- 포터 스테머(Porter Stemmer)와 같은 알고리즘이 사용됩니다.

어근 추출 (Lemmatization):
- 단어의 어근을 추출하여 형태를 표준화합니다. 어간 추출보다 더 정교합니다.
- 예: "running", "ran" → "run"
- 품사 정보를 사용하여 정확한 어근을 찾아냅니다. 예를 들어, "better"는 어근 "good"으로 변환됩니다.
- WordNetLemmatizer와 같은 도구가 사용됩니다.

BPE (Byte Pair Encoding):
- 자주 등장하는 바이트 쌍을 병합하여 점진적으로 단어를 분해합니다.
- 예: "lowest"가 "l", "o", "w", "e", "s", "t"로 분해되고, 자주 등장하는 "lo", "we"가 결합되어 "low", "est"로 변환됩니다.
- BPE는 신경망 번역 모델과 같은 대규모 언어 모델에서 널리 사용됩니다.

WordPiece:
- BPE와 유사하지만, 서브워드(subword) 단위로 토큰을 생성합니다.
- 예: "unhappiness" → ["un", "##happiness"]
- 트랜스포머 모델(BERT 등)에서 사용됩니다.

SentencePiece:
- 언어에 중립적인 방식으로 텍스트를 서브워드 단위로 분해합니다.
- BPE와 유사하지만, 문장을 토큰화하는 과정에서 공백을 고려하지 않음.
- 예: "unhappiness" → ["un", "ha", "ppiness"]
- Google의 T5, ALBERT 모델에서 사용됩니다.

#################################################################### text_to_word_sequence 함수
## text_to_word_sequence 함수 call
from tensorflow.keras.preprocessing.text import text_to_word_sequence

## 전처리할 Text
text = '해보지 않으면 해낼 수 없다'

## 해당 텍스트 토큰화
result = text_to_word_sequence(text)
print("\n원문:\n", text)
print("\n토큰화:\n", result)

#################################################################### Tokenizer 클래스
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

docs=['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',
      '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',
      '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다.',
      ]

token=Tokenizer()
token.fit_on_texts(docs)

## 각 옵션에 맞추어 출력
print('\n단어 수: \n',token.word_counts)
print('\n문장 수: \n',token.document_count)
print('\n각 단어별 문장에 포함되어 있는 개수: \n',token.word_docs)
print('\n각 단어에 매겨진 인텍스 값: \n',token.word_index)

















#####################################################################################################
#############################################   Examples  ###########################################
##################################################################################################### 





#################################################################### 
#################################################################### Q.
